{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training a Classification Model Using Labelchecker Data**\n",
    "Here, we walk through the of training a classification model using the small dataset provided, demonstrating the essential steps of the process. Keep in mind that the modelâ€™s performance may be limited due to the small training data, but you can easily adapt our example to your own dataset.\n",
    "\n",
    "Hereâ€™s an overview of what weâ€™ll cover:\n",
    "\n",
    "1. **Data Download**: Obtain the example data.\n",
    "2. **Data Preparation**: Detail the necessary processing steps before training.\n",
    "3. **Model Building**: Construct the classification model.\n",
    "4. **Data Loading**: Set up data loaders for model training.\n",
    "5. **Model Training**: Train the model.\n",
    "6. **Model Evaluation**: Assess its performance.\n",
    "7. **Model Serialization**: Save the trained model for future use.\n",
    "\n",
    "Feel free to replace our example data with your own to train a model tailored to your specific needs ðŸ˜Ž. Letâ€™s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import cv2\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import zipfile\n",
    "from rich import print\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "from enum import StrEnum, auto\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from plotly import express as px\n",
    "from plotly import graph_objects as go\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Data Import**\n",
    "Let's import some data and start exploring it!\n",
    "\n",
    "You could do this with the [Example data](https://www.dropbox.com/s/4p1e5j9p9v8xj2s/data.zip?dl=1]) as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the data\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "# set dataset name\n",
    "dataset_name = \"example\"\n",
    "data_path = data_path.joinpath(dataset_name)\n",
    "# make sure the data directory and subdirectories exists\n",
    "data_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all Labelchecker data files from the data directory\n",
    "data_files = list(data_path.glob(f\"**/LabelChecker_*.csv\"))\n",
    "print(f\"Found {len(data_files)} data files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Data Preparation and data cleaning**\n",
    "The goal is to streamline the process and ensure consistency across all data files before we train the model.  \n",
    "\n",
    "To clean and prepare the data we:\n",
    "1. Subset data that has a `LabelTrue value`\n",
    "2. drop columns with only `missing values`\n",
    "3. drop columns with `default values`\n",
    "4. set `image paths`\n",
    "5. drop columns with `object` data\n",
    "6. remove labels with less than N examples\n",
    "7. `encode` label\n",
    "\n",
    "All this is done in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for default values function\n",
    "def is_default(series: pd.Series) -> bool:\n",
    "    return len(series.unique()) == 1\n",
    "\n",
    "\n",
    "# drop all object columns except for LabelTrue function\n",
    "def is_object(\n",
    "    series: pd.Series,\n",
    "    columns_to_keep: list[str] = [\"LabelTrue\", \"ImageFilename\", \"CollageFile\"],\n",
    ") -> bool:\n",
    "    if series.name in columns_to_keep:\n",
    "        return False\n",
    "    return series.dtype == \"object\"\n",
    "\n",
    "# drop labels with less than N examples\n",
    "def drop_labels_with_less_than_examples(data: pd.DataFrame, min_examples: int) -> pd.DataFrame:\n",
    "    return data.groupby(\"LabelTrue\").filter(lambda x: len(x) >= min_examples)\n",
    "\n",
    "# build image paths\n",
    "def build_image_path(df: pd.DataFrame, directory: Path) -> Tuple[bool, list[str]]:\n",
    "    \"\"\"\n",
    "    Builds a list of image paths based on the given DataFrame and directory.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the image filenames and names.\n",
    "        directory (Path): The directory where the images are located.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[bool, list[str]]: A tuple containing a boolean value indicating whether the image paths are for collage files,\n",
    "        and a list of image paths.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If any of the image files are missing.\n",
    "    \"\"\"\n",
    "    is_collage = True\n",
    "    image_paths = []\n",
    "    if \"ImageFilename\" in df.columns:\n",
    "        if not df[\"ImageFilename\"].isnull().all() and not df[\"Name\"].isnull().all():\n",
    "            is_collage = False\n",
    "            for name, filename in zip(df[\"Name\"], df[\"ImageFilename\"]):\n",
    "                image_path = Path.joinpath(directory, name, filename)\n",
    "                if not image_path.exists():\n",
    "                    raise FileNotFoundError(f\"file {filename} not found\")\n",
    "                image_paths.append(image_path.as_posix())\n",
    "    if \"CollageFile\" in df.columns:\n",
    "        if not df[\"CollageFile\"].isnull().all():\n",
    "            is_collage = True\n",
    "            for collage_file in df[\"CollageFile\"]:\n",
    "                image_path = Path.joinpath(directory, collage_file)\n",
    "                if not image_path.exists():\n",
    "                    raise FileNotFoundError(f\"file {collage_file} not found\")\n",
    "                image_paths.append(image_path.as_posix())\n",
    "    return is_collage, image_paths\n",
    "\n",
    "\n",
    "def load_training_data(\n",
    "    data_files: list[Path], \n",
    "    encoder: LabelEncoder, \n",
    "    min_examples: int = 5,\n",
    ") -> Tuple[pd.DataFrame, LabelEncoder]:\n",
    "    \"\"\"\n",
    "    Load the training data from the data files, preprocess the data, and encode the labels.\n",
    "\n",
    "    Args:\n",
    "        data_files (list[Path]): A list of file paths to the training data files.\n",
    "        encoder (LabelEncoder): An instance of the LabelEncoder class used for label encoding.\n",
    "        min_examples (int, optional): The minimum number of examples required for each label. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, LabelEncoder]: A tuple containing the preprocessed training data as a DataFrame\n",
    "        and the label encoder object.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for data_file in data_files:\n",
    "        if not data_file.exists():\n",
    "            raise FileNotFoundError(f\"File {data_file} not found\")\n",
    "        df = pd.read_csv(data_file)\n",
    "\n",
    "        # Build the image paths\n",
    "        is_collage, image_paths = build_image_path(df, data_file.parent)\n",
    "        if image_paths:\n",
    "            if is_collage:\n",
    "                df[\"CollageFile\"] = image_paths\n",
    "            else:\n",
    "                df[\"ImageFilename\"] = image_paths\n",
    "        data.append(df)\n",
    "    data = pd.concat(data)\n",
    "\n",
    "    # Drop rows with missing LabelTrue values\n",
    "    data = data.loc[data[\"LabelTrue\"].str.len() > 0]\n",
    "    data = data.dropna(subset=[\"LabelTrue\"])\n",
    "\n",
    "    # Drop columns with all missing values\n",
    "    data = data.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Drop columns with default values\n",
    "    data = data.loc[:, ~data.apply(is_default)]\n",
    "\n",
    "    # Drop all object columns except for LabelTrue function\n",
    "    data = data.loc[:, ~data.apply(is_object)]\n",
    "\n",
    "    # Drop labels with less than N examples\n",
    "    data = drop_labels_with_less_than_examples(data, min_examples=min_examples)\n",
    "    \n",
    "    # Drop ProbabilityScore column\n",
    "    data = data.drop('ProbabilityScore', axis=1)\n",
    "\n",
    "    # Encode the labels\n",
    "    data[\"LabelTrue\"] = encoder.fit_transform(data[\"LabelTrue\"])\n",
    "    return (data, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to print label counts\n",
    "def print_label_counts(data: pd.DataFrame, class_names: list[str]):\n",
    "    label_counts = data[\"LabelTrue\"].value_counts()\n",
    "    value_counts = {}\n",
    "    for label, count in label_counts.items():\n",
    "        value_counts[class_names[label]] = count\n",
    "\n",
    "    # sort the labels by count\n",
    "    sorted_value_counts = sorted(\n",
    "        value_counts.items(), key=lambda x: x[1], reverse=False\n",
    "    )\n",
    "    sorted_labels = [label for label, count in sorted_value_counts]\n",
    "    sorted_counts = [count for label, count in sorted_value_counts]\n",
    "\n",
    "    # plot the label counts\n",
    "    px.bar(\n",
    "        x=sorted_counts,\n",
    "        y=sorted_labels,\n",
    "        title=\"Label Counts\",\n",
    "        orientation=\"h\",\n",
    "        labels={\"x\": \"Count\", \"y\": \"Label\"},\n",
    "        width=800,\n",
    "        height=1200,\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the label encoder\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data\n",
    "training_data, encoder = load_training_data(data_files, encoder)\n",
    "print(f\"the training data contains {training_data.shape[0]} samples\")\n",
    "print(\n",
    "    f\"the training data contains the following columns: {[column_name for column_name in training_data.columns]}\"\n",
    ")\n",
    "print(\n",
    "    f\"the training data contains these labels: {encoder.classes_}; \\na total of {len(encoder.classes_)} labels\"\n",
    ")\n",
    "print_label_counts(training_data, encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Model Building**\n",
    "We're going to train a model that uses the object images and features to classify the object class. \n",
    "\n",
    "Lets start with designing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "def create_model(\n",
    "    features_input_shape: tuple[int],\n",
    "    image_input_shape: tuple[int, int, int],\n",
    "    nr_classes: int,\n",
    "    optimizer: tf.keras.optimizers.Optimizer = Adam(),\n",
    "    loss: tf.keras.losses.Loss = SparseCategoricalCrossentropy(),\n",
    "    metric: tf.keras.metrics.Metric = SparseCategoricalAccuracy(),\n",
    "    features_normalization: layers.Normalization = None,\n",
    "    image_augmentation: tf.keras.Sequential = None,\n",
    ") -> tf.keras.Model:\n",
    "    # Multi layer perceptron model\n",
    "    features_input = layers.Input(shape=features_input_shape, name=\"features\")\n",
    "    if features_normalization:\n",
    "        x1 = features_normalization(features_input)\n",
    "        x1 = layers.Dense(344, activation=\"relu\", name=\"dense_10\")(x1)\n",
    "    else:\n",
    "        x1 = layers.Dense(344, activation=\"relu\", name=\"dense_10\")(features_input)\n",
    "    x1 = layers.Dropout(0.2, name=\"dropout_10\")(x1)\n",
    "    x1 = layers.Dense(172, activation=\"relu\", name=\"dense_11\")(x1)\n",
    "    x1 = layers.Dropout(0.15, name=\"dropout_11\")(x1)\n",
    "    x1 = layers.Dense(86, activation=\"relu\", name=\"dense_12\")(x1)\n",
    "\n",
    "    # Convolution Neural Network model\n",
    "    image_input = layers.Input(shape=image_input_shape, name=\"image\")\n",
    "    x2 = layers.Rescaling(1.0 / 255)(image_input)\n",
    "    if image_augmentation:\n",
    "        x2 = image_augmentation(x2)\n",
    "    x2 = layers.Conv2D(16, 1, activation=\"relu\", padding=\"same\", name=\"conv2d_10\")(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=1, padding=\"same\", name=\"max_pooling2d_10\")(x2)\n",
    "    x2 = layers.Conv2D(32, 1, activation=\"relu\", padding=\"same\", name=\"conv2d_11\")(x2)\n",
    "    x2 = layers.Conv2D(64, 1, activation=\"relu\", padding=\"same\", name=\"conv2d_12\")(x2)\n",
    "    x2 = layers.GlobalAveragePooling2D(name=\"global_average_pooling2d_10\")(x2)\n",
    "\n",
    "    # concatenate MLP and CNN models\n",
    "    x = layers.concatenate([x1, x2], name=\"concatenate_20\")\n",
    "    x = layers.Dense(500, activation=\"relu\", name=\"dense_20\")(x)\n",
    "    x = layers.Dropout(0.1, name=\"dropout_20\")(x)\n",
    "    output = layers.Dense(nr_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    # create the model\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[features_input, image_input], outputs=output\n",
    "    )  # note the order of the inputs\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Data loader**\n",
    "Now that weâ€™ve loaded and prepared the data, making it ready for training, we need to set up a data loader that can load the each image, retrieving its features, and obtaining its label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  read the image file\n",
    "def decode_image(row: pd.Series, image_size: tuple[int, int, int]) -> tf.Tensor:\n",
    "    if \"ImageFilename\" in row:\n",
    "        image_string = tf.io.read_file(row[\"ImageFilename\"])\n",
    "        image = tf.io.decode_png(image_string, channels=image_size[-1])  # png images\n",
    "        return image\n",
    "    else:\n",
    "        image_path = tf.strings.as_string(row[\"CollageFile\"])\n",
    "        image = tf.numpy_function(read_tiff, [image_path], tf.uint8)\n",
    "        image.set_shape([None, None, 3])\n",
    "        image = remove_alpha_channel(\n",
    "            image, image_size=image_size\n",
    "        )  # RGBA (4 channels) to RGB (3 channels)\n",
    "        image = crop_image(row, image)  # crop out the object image\n",
    "        return image\n",
    "\n",
    "# read TIFF images\n",
    "def read_tiff(path_tensor: tf.Tensor):\n",
    "    # path_tensor is already bytes, just decode it\n",
    "    path = path_tensor.decode(\"utf-8\")\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image not found at path: {path}\")\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "# remove the alpha channel\n",
    "def remove_alpha_channel(image, image_size: tuple[int, int, int]) -> tf.Tensor:\n",
    "    return tf.convert_to_tensor(image[:, :, : image_size[-1]])  # remove alpha channel\n",
    "\n",
    "\n",
    "# crop out the object image from the collage\n",
    "def crop_image(row: pd.Series, image):\n",
    "    image_x = tf.squeeze(row[\"ImageX\"])\n",
    "    image_y = tf.squeeze(row[\"ImageY\"])\n",
    "    image_width = tf.squeeze(row[\"ImageW\"])\n",
    "    image_height = tf.squeeze(row[\"ImageH\"])\n",
    "    return image[\n",
    "        int(image_y) : int(image_y) + int(image_height),\n",
    "        int(image_x) : int(image_x) + int(image_width),\n",
    "    ]\n",
    "\n",
    "\n",
    "def resize_image(image, image_size: tuple[int, int, int]) -> tf.Tensor:\n",
    "    image = tf.image.resize(image, [image_size[0], image_size[1]])  # H, W only\n",
    "    return image\n",
    "\n",
    "\n",
    "# combining all the image processing functions\n",
    "def get_image(row: pd.Series, image_size: tuple[int, int, int]) -> tf.Tensor:\n",
    "    image = decode_image(row, image_size=image_size)\n",
    "    return resize_image(image, image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object features\n",
    "def get_features(row: pd.Series, feature_names: list[str]) -> tf.Tensor:\n",
    "    return tf.convert_to_tensor(\n",
    "        [float(row[feature]) for feature in feature_names], dtype=tf.float64\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "def get_label(row: pd.Series):\n",
    "    return row.pop(\"LabelTrue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "    row: pd.Series,\n",
    "    image_size: Tuple[int, int, int],\n",
    "    feature_names: list[str],\n",
    "):\n",
    "    image = get_image(row, image_size=image_size)\n",
    "    features = get_features(row, feature_names=feature_names)\n",
    "    label = get_label(row)\n",
    "    return (\n",
    "        features,\n",
    "        image,\n",
    "    ), label  # Note: the order of the features and image is important for the model input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 *Data loader parameters*\n",
    "To initialize the dataloader we have to set a few parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the image size\n",
    "image_size = (32, 32, 3)\n",
    "\n",
    "# set the object describing feature names\n",
    "feature_names = [\n",
    "    column_name\n",
    "    for column_name in training_data.select_dtypes(exclude=\"object\").columns\n",
    "    if column_name\n",
    "    not in [\n",
    "        \"LabelTrue\",  # this is the target column and no longer a \"object\" data type but integer\n",
    "        \"Id\",\n",
    "        \"CalImage\",\n",
    "        \"ElapsedTime\",\n",
    "        \"ImageY\",\n",
    "        \"ImageX\",\n",
    "        \"ImageW\",\n",
    "        \"ImageH\",\n",
    "        \"IntensityCalimage\",\n",
    "        \"SrcX\",\n",
    "        \"SrcY\",\n",
    "        \"SrcImage\",\n",
    "    ]\n",
    "]\n",
    "feature_size = len(feature_names)\n",
    "nr_of_classes = len(encoder.classes_)\n",
    "\n",
    "# selected features check\n",
    "print(f\"Selected features: {feature_names}\")\n",
    "print(f\"Number of selected features: {feature_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 *Train-test split*\n",
    "We need to split the data in train and test data. The test data we use for validating the model during training and to detect any model overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(\n",
    "    training_data, stratify=training_data[\"LabelTrue\"], test_size=0.2, random_state=42\n",
    ")  # we set the random state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 22\n",
    "\n",
    "# create the training datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(dict(X_train))\n",
    "train_ds = train_ds.map(\n",
    "    lambda x: get_data(x, image_size=image_size, feature_names=feature_names),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "train_ds = (\n",
    "    train_ds.shuffle(buffer_size=1024)\n",
    "    .batch(batch_size=batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test dataset\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(dict(X_test))\n",
    "test_ds = test_ds.map(\n",
    "    lambda x: get_data(x, image_size=image_size, feature_names=feature_names),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "test_ds = test_ds.batch(batch_size=batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if train_ds is empty\n",
    "if train_ds:\n",
    "    # Inspect the first batch of the train\n",
    "    for (features, images), labels in train_ds.take(1):\n",
    "        print(f\"image shape: {images.shape}\")\n",
    "        print(f\"features shape: {features.shape}\")\n",
    "        print(f\"label shape: {labels.shape}\")\n",
    "\n",
    "        # plot the images\n",
    "        plt.figure(figsize=(10, 10), frameon=False)\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.title(encoder.classes_[labels[i]], color=\"lightgreen\")\n",
    "            plt.axis(\"off\")\n",
    "else:\n",
    "    print(\"train_ds is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if val_ds is empty\n",
    "if test_ds:\n",
    "    # Iterate over the dataset\n",
    "    for (features, image), label in test_ds.take(1):\n",
    "        print(f\"image shape: {image.shape}\")\n",
    "        print(f\"features shape: {features.shape}\")\n",
    "        print(f\"label shape: {label.shape}\")\n",
    "else:\n",
    "    print(\"val_ds is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. *Initialize feature normalization layer*\n",
    "We need to set the mean and standard deviation for the normalization layer before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and set the normalization layer\n",
    "def create_normalization_layer(\n",
    "    data: pd.DataFrame, features: list[str]\n",
    ") -> layers.Normalization:\n",
    "    # features = data[features].to_numpy(dtype=np.float64)\n",
    "    normalization_layer = layers.Normalization()\n",
    "    normalization_layer.adapt(\n",
    "        tf.convert_to_tensor(data[features].to_numpy(), dtype=tf.float64)\n",
    "    )\n",
    "    return normalization_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the normalization layer\n",
    "features_normalization = create_normalization_layer(X_train, feature_names)\n",
    "features_normalization(X_train[feature_names].to_numpy()[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. *Initialize the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = create_model(\n",
    "    features_input_shape=(feature_size,),\n",
    "    image_input_shape=image_size,\n",
    "    nr_classes=nr_of_classes,\n",
    "    features_normalization=features_normalization,\n",
    "    optimizer=Adam(learning_rate=0.001) if sys.platform == \"win32\" else Adam(learning_rate=0.001, clipnorm=1.0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model summary\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "# note: you need to install Graphviz for the plot to work (https://graphviz.gitlab.io/download/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. *Callbacks*\n",
    "A callback performs actions at various stages of training. We're going to use two callbacks, namely:\n",
    "- **ModelCheckpoint** save the model after each epoch when the model has improved; and\n",
    "- **EarlyStopping** stop the training of the model when classification performance did not increase for 6 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_version(models_dir: Path, model_name: str) -> Path:\n",
    "    \"\"\"\n",
    "    Get the version of the model based on the existing models in the model directory.\n",
    "\n",
    "    Args:\n",
    "        model_dir (Path): The directory where the models are stored.\n",
    "        model_name (str): The name of the model.\n",
    "\n",
    "    Returns:\n",
    "        Path\n",
    "    \"\"\"\n",
    "    version = 1\n",
    "    model_path = Path.joinpath(models_dir, model_name, str(version))\n",
    "    while model_path.exists():\n",
    "        version += 1\n",
    "        model_path = Path.joinpath(models_dir, model_name, str(version))\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be saving the models directly into the Classification service, found at this location:\n",
    "\n",
    "```bash\n",
    "|--src\n",
    "    |-- services\n",
    "        |--classification\n",
    "            |--ObjectClassifcation\n",
    "                |--models\n",
    "                    |--<model_name>\n",
    "                        |--<model_version>\n",
    "                            |--config.json\n",
    "                            |--<serialized_model>\n",
    "                |--...\n",
    "    |--....\n",
    "|--main.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to service\n",
    "path_to_service = Path().joinpath(\"src\", \"services\", \"classification\")\n",
    "service_name = \"ObjectClassification\"\n",
    "\n",
    "# set the path to the models directory\n",
    "models_dir = Path().joinpath(path_to_service, service_name, \"models\")\n",
    "model_name = \"Example\" \n",
    "model_dir = get_model_version(models_dir, model_name)\n",
    "print(f\"Model directory: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint callback\n",
    "checkpoint_filepath = Path.joinpath(model_dir, \"checkpoint\", \"checkpoint.weights.h5\")\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath.as_posix(),\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_sparse_categorical_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "checkpoint_dir = checkpoint_filepath.parent\n",
    "print(f\"Model checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_sparse_categorical_accuracy\",\n",
    "    patience=6,\n",
    "    min_delta=0.001,\n",
    "    mode=\"max\",\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 *Train the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[model_checkpoint_callback, early_stopping_callback],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# note: make sure your computer doesn't go into sleep-mode while training! The process will stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. **Model evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history\n",
    "def plot_training_history(history: tf.keras.callbacks.History):\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.loc[:, [\"loss\", \"val_loss\"]].plot(title=\"Loss\")\n",
    "    history_df.loc[\n",
    "        :, [\"sparse_categorical_accuracy\", \"val_sparse_categorical_accuracy\"]\n",
    "    ].plot(title=\"Accuracy\")\n",
    "\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights from best model checkpoint\n",
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, acc = model.evaluate(test_ds, verbose=2)\n",
    "print(\"Trained model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = model.predict(test_ds)\n",
    "predicted_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "def plot_confusion_matrix(\n",
    "    true_labels: np.ndarray,\n",
    "    predicted_labels: np.ndarray,\n",
    "    class_names: list[str],\n",
    "    text_size: int = 10,\n",
    "    normalize: bool = True,\n",
    "    width: int = 1000,\n",
    "    height: int = 1000,\n",
    "):\n",
    "    cm = confusion_matrix(\n",
    "        y_true=true_labels,\n",
    "        y_pred=predicted_labels,\n",
    "        normalize=\"true\" if normalize else None,\n",
    "    )\n",
    "    # normalize the confusion matrix\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=cm,\n",
    "            x=class_names,\n",
    "            y=class_names,\n",
    "            colorscale=\"Viridis\",\n",
    "            showscale=False,\n",
    "            text=cm,\n",
    "            texttemplate=\"%{text:.2f}\",\n",
    "            textfont={\"size\": text_size},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Confusion Matrix\",\n",
    "        title_x=0.5,\n",
    "        xaxis_title=\"Predicted\",\n",
    "        yaxis_title=\"True\",\n",
    "        autosize=False,\n",
    "        width=width,\n",
    "        height=height,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    predicted_labels=predicted_labels,\n",
    "    true_labels=X_test[\"LabelTrue\"],\n",
    "    class_names=encoder.classes_,\n",
    "    text_size=10,\n",
    "    normalize=True,\n",
    "    width = 1000,\n",
    "    height = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification report\n",
    "print(\n",
    "    classification_report(\n",
    "        X_test[\"LabelTrue\"], predicted_labels, target_names=encoder.classes_, zero_division=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. **Model Serialization**\n",
    "We've trained the model, evaluated it and now it's time to save it, or serialize it.  \n",
    "We save:\n",
    "- the model itself\n",
    "- the model configuration\n",
    "- the evaluation output ???\n",
    "\n",
    "let's start with the model configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. *Model configuration*\n",
    "Often the first model we train is not the one we end up using. We might want to add more data, test specific features, increase the image size, increase model size, etc. Whatever we change, we want our model to work, so we need to save these choices in a `configuration` file. To do this we'll use a python class which we save as a .json file.\n",
    "\n",
    "The python class is called `ModelConfig` and in this configuration class we save the following information:\n",
    "- **name**: name of the model\n",
    "- **version**: which version of the model\n",
    "- **framework**: e.g. Tensorflow\n",
    "- **Class_names**: the encoder classes to be able to translate predicted numbers back to labels\n",
    "- **Input_shape**: input shape of the image\n",
    "- **Features**: list of features we used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelConfig class\n",
    "class ModelConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        version: str,\n",
    "        framework: str,\n",
    "        class_names: list[str],\n",
    "        input_shape: Tuple[int, int, int],\n",
    "        features: list[str],\n",
    "    ) -> None:\n",
    "        self.Name: str = name\n",
    "        self.Version: str = version\n",
    "        self.Framework: str = framework\n",
    "        self.Class_names: list[str] = class_names\n",
    "        self.Input_shape: list[int] = list(input_shape)\n",
    "        self.Features: list[str] = features\n",
    "\n",
    "        # Check if any value is None\n",
    "        if any(value is None for value in self.__dict__.values()):\n",
    "            raise ValueError(\"Not all values have been initialized\")\n",
    "\n",
    "    # representation of the class\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"ModelConfig(Name={self.Name},\\n Version={self.Version},\\n Framework={self.Framework},\\n Class_names={self.Class_names},\\n Input_shape={self.Input_shape},\\n Features={self.Features})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frameworks(StrEnum):\n",
    "    TENSORFLOW = auto()\n",
    "    PYTORCH = auto()\n",
    "    ONNX = auto() \n",
    "    SKOPS = auto() # scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model configurations\n",
    "model_configurations = ModelConfig(\n",
    "    name=model_name,\n",
    "    version=model_dir.name,\n",
    "    framework=Frameworks.TENSORFLOW,\n",
    "    class_names=list(encoder.classes_),\n",
    "    input_shape=image_size,\n",
    "    features=feature_names,\n",
    ")\n",
    "print(model_configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the model directory exists\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save model configuration to json\n",
    "model_config_file = Path.joinpath(model_dir, \"config.json\")\n",
    "with open(model_config_file, \"w\") as f:\n",
    "    json.dump(model_configurations.__dict__, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. *Save model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and configuration file are saved in a folder with the model version in the 'model_name' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: tf.keras.Model, model_dir: Path, model_suffix: str):\n",
    "    # ensure the model directory exists\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if model_suffix == \"keras\":\n",
    "        model_path = Path.joinpath(model_dir, f\"model.{model_suffix}\")\n",
    "        print(f\"Model path: {model_path}\")\n",
    "        model.save(model_path.as_posix())\n",
    "    elif model_suffix in [\"h5\", \"hdf5\"]:  # legacy formats\n",
    "        model_path = Path.joinpath(model_dir, f\"model.{model_suffix}\")\n",
    "        print(f\"Model path: {model_path}\")\n",
    "        model.save(model_path.as_posix())\n",
    "        shutil.copy(\n",
    "            checkpoint_filepath,\n",
    "            Path.joinpath(model_dir, f\"model.weights.{model_suffix}\"),\n",
    "        )  # needed for legacy formats and must be loaded in after loading the model for classification\n",
    "    else:\n",
    "        raise ValueError(f\"Model suffix {model_suffix} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_suffix = \"keras\"\n",
    "save_model(model, model_dir, model_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional resources:\n",
    "For much of this notebook we used inspiration from theses tensorflow tutorials:\n",
    "1. [Load and preprocess images](https://www.tensorflow.org/tutorials/load_data/images)\n",
    "2. [Load a Pandas dataframe](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
